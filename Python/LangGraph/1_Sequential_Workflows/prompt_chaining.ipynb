{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceeaebba",
   "metadata": {},
   "source": [
    "Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48686ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:05<00:00, 32.54s/it]\n",
      "Device set to use cuda:0\n",
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and Treatment:**\n",
      "        * Explain how AI can analyze medical images (X-rays, CT scans, etc.) to detect diseases earlier and more accurately.\n",
      "        * Discuss AI-powered tools for personalized medicine, tailoring treatments to individual patients.\n",
      "        * Highlight the potential of AI in drug discovery and development.\n",
      "    * **B. Enhanced Efficiency and Accessibility:**\n",
      "        * Explain how AI can automate administrative tasks, freeing up healthcare professionals for patient care.\n",
      "        * Discuss the potential of\n"
     ]
    }
   ],
   "source": [
    "# ✅ Imports\n",
    "from langgraph.graph import StateGraph, START, END   # LangGraph workflow builder\n",
    "from langchain_huggingface import HuggingFacePipeline  # Hugging Face → LangChain wrapper\n",
    "from typing import TypedDict   # Define structured state schema\n",
    "from dotenv import load_dotenv # Load env variables (e.g., Hugging Face token)\n",
    "\n",
    "import torch, gc\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Free GPU memory before loading the model\n",
    "# ---------------------------------------------------\n",
    "gc.collect()                  # Run Python garbage collector\n",
    "torch.cuda.empty_cache()      # Release cached but unused GPU memory\n",
    "torch.cuda.ipc_collect()      # Clean up interprocess GPU memory handles\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Load environment variables\n",
    "# ---------------------------------------------------\n",
    "# This pulls values from your .env file (like HF API token).\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Choose and wrap an LLM model\n",
    "# ---------------------------------------------------\n",
    "gemma_model = \"google/gemma-2-2b-it\"  # Instruction-tuned Gemma model\n",
    "\n",
    "# HuggingFacePipeline wraps a HF text-generation pipeline\n",
    "# so it behaves like a LangChain LLM object.\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=gemma_model,\n",
    "    task=\"text-generation\",   # Pipeline type\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,      # Lower = more deterministic\n",
    "        max_new_tokens=100,   # Limit output length\n",
    "        return_full_text=False # Return only generated text (not prompt+text)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Define the workflow state schema\n",
    "# ---------------------------------------------------\n",
    "# Each node in the LangGraph will pass this dictionary around.\n",
    "class BlogState(TypedDict):\n",
    "    title: str    # Input: blog post title\n",
    "    outline: str  # Intermediate: outline generated from title\n",
    "    content: str  # Final: full blog content\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Define workflow nodes\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Node 1: create_outline\n",
    "def create_outline(state: BlogState) -> BlogState:\n",
    "    # Get the title from state\n",
    "    title = state['title']\n",
    "    \n",
    "    # Prompt the model to generate a blog outline\n",
    "    prompt = f\"Create a detailed outline for a blog post titled: {title}\"\n",
    "    \n",
    "    outline = model.invoke(prompt)   # Call the model\n",
    "    \n",
    "    state['outline'] = outline       # Save output in state\n",
    "    return state\n",
    "\n",
    "\n",
    "# Node 2: create_blog\n",
    "def create_blog(state: BlogState) -> BlogState:\n",
    "    # Get inputs from state\n",
    "    title = state['title']\n",
    "    outline = state['outline']\n",
    "    \n",
    "    # Prompt the model to generate full blog content based on the outline\n",
    "    prompt = f\"Write a detailed blog post based on the title: {title} and the outline: {outline}\"\n",
    "    \n",
    "    content = model.invoke(prompt)\n",
    "    \n",
    "    state['content'] = content       # Save output in state\n",
    "    return state\n",
    "\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Build the workflow graph\n",
    "# ---------------------------------------------------\n",
    "graph = StateGraph(BlogState)       # Initialize with our state schema\n",
    "\n",
    "# Register nodes\n",
    "graph.add_node('create_outline', create_outline)\n",
    "graph.add_node('create_blog', create_blog)\n",
    "\n",
    "# Define execution flow:\n",
    "# START → create_outline → create_blog → END\n",
    "graph.add_edge(START, 'create_outline')\n",
    "graph.add_edge('create_outline', 'create_blog')\n",
    "graph.add_edge('create_blog', END)\n",
    "\n",
    "# Compile graph into an executable workflow\n",
    "workflow = graph.compile()\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 7. Run the workflow\n",
    "# ---------------------------------------------------\n",
    "initial_state = {'title': \"The Future of AI in Healthcare\"}  # Input to workflow\n",
    "\n",
    "# Run graph execution\n",
    "final_state = workflow.invoke(initial_state)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 8. Show final blog content\n",
    "# ---------------------------------------------------\n",
    "print(final_state['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9aa944a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": Promises and Perils.\n",
      "\n",
      "**I. Introduction**\n",
      "    * Hook: Start with a compelling statistic or anecdote about AI's impact on healthcare.\n",
      "    * Briefly introduce the topic: AI's growing role in healthcare and its potential benefits and risks.\n",
      "    * Thesis statement: AI holds immense promise for revolutionizing healthcare, but its implementation requires careful consideration of ethical and practical challenges.\n",
      "\n",
      "**II. The Promise of AI in Healthcare**\n",
      "    * **A. Improved Diagnosis\n"
     ]
    }
   ],
   "source": [
    "print(final_state['outline'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a924e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8b1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
