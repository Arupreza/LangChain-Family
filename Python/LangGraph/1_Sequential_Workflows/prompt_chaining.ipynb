{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceeaebba",
   "metadata": {},
   "source": [
    "Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48686ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StateFraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     24\u001b[0m     outline: \u001b[38;5;28mstr\u001b[39m\n\u001b[1;32m     25\u001b[0m     content: \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m---> 28\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mStateFraph\u001b[49m(BlogState)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_outline\u001b[39m(state: BlogState) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BlogState:\n\u001b[1;32m     33\u001b[0m     title \u001b[38;5;241m=\u001b[39m state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StateFraph' is not defined"
     ]
    }
   ],
   "source": [
    "# ✅ Imports\n",
    "from langgraph.graph import StateGraph, START, END   # StateGraph = workflow builder\n",
    "from langchain_huggingface import HuggingFacePipeline  # wrapper to use HF models in LangChain\n",
    "from typing import TypedDict   # define workflow state schema\n",
    "from dotenv import load_dotenv # load env variables (like Hugging Face token)\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "gemma_model = \"google/gemma-2-2b-it\" \n",
    "\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=gemma_model,\n",
    "    task=\"text-generation\",   # type of pipeline we want\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,      # controls randomness (lower = deterministic)\n",
    "        max_new_tokens=100,   # maximum tokens to generate per response\n",
    "        return_full_text=False # return only the answer (not the prompt+answer)\n",
    "    ),\n",
    ")\n",
    "\n",
    "class BlogState(TypedDict):\n",
    "    \n",
    "    title: str\n",
    "    outline: str\n",
    "    content: str\n",
    "    \n",
    "    \n",
    "graph = StateGraph(BlogState)\n",
    "\n",
    "\n",
    "def create_outline(state: BlogState) -> BlogState:\n",
    "\n",
    "    title = state['title']\n",
    "    \n",
    "    prompt = f\"Create a detailed outline for a blog post titled: {title}\"\n",
    "    \n",
    "    outline = model.invoke(prompt)\n",
    "    \n",
    "    state['outline'] = outline\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_blog(state: BlogState) -> BlogState:\n",
    "    \n",
    "    title = state['title']\n",
    "    outline = state['outline']\n",
    "    \n",
    "    prompt = f\"Write a detailed blog post based on the title: {title} and the outline: {outline}\"\n",
    "    \n",
    "    content = model.invoke(prompt)\n",
    "    \n",
    "    state['content'] = content\n",
    "    \n",
    "    return state\n",
    "\n",
    "\n",
    "graph.add_node('create_outline', create_outline)\n",
    "graph.add_node('create_blog', create_blog)\n",
    "\n",
    "\n",
    "graph.add_edge(START, 'create_outline')\n",
    "graph.add_edge('create_outline', 'create_blog')\n",
    "graph.add_edge('create_blog', END)  \n",
    "\n",
    "workflow = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f82439",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aa944a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "\n",
    "# Delete any model or large tensors you no longer need\n",
    "del model  # or any other big object\n",
    "gc.collect()  # Python garbage collection\n",
    "torch.cuda.empty_cache()  # clears cached memory\n",
    "torch.cuda.ipc_collect()  # clears interprocess memory handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a924e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c8b1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
