{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceeaebba",
   "metadata": {},
   "source": [
    "Prompt Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48686ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n",
      "Device set to use cuda:0\n",
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OUTLINE ===\n",
      " \n",
      "\n",
      "**Blog Title:** The Future of AI in Healthcare\n",
      "\n",
      "**Outline:**\n",
      "\n",
      "**H1: The Future of AI in Healthcare**\n",
      "\n",
      "**H2: Transforming Patient Care**\n",
      "    * H3: Personalized Medicine and Treatment\n",
      "    * H3: Early Disease Detection and Prevention\n",
      "    * H3: Improved Diagnosis Accuracy\n",
      "    * H3: Enhanced Patient Engagement and Communication\n",
      "    * H3: Remote Patient Monitoring and Management\n",
      "\n",
      "**H2: Revolutionizing Healthcare Operations**\n",
      "    * H3: Streamlining Administrative Tasks\n",
      "    * H3: Optimizing Resource Allocation\n",
      "    * H3: Automating Medical Procedures\n",
      "    * H3: Reducing Healthcare Costs\n",
      "\n",
      "**H2: Ethical Considerations and Challenges**\n",
      "    * H3: Data Privacy and Security\n",
      "    * H3: Algorithmic Bias and Fairness\n",
      "    * H3: Transparency and Explainability\n",
      "    \n",
      "\n",
      "=== CONTENT ===\n",
      " \n",
      "\n",
      "**H1: The Future of AI in Healthcare**\n",
      "\n",
      "The healthcare industry is on the cusp of a revolution, driven by the rapid advancements in artificial intelligence (AI). AI is poised to transform patient care, revolutionize healthcare operations, and address critical ethical considerations. This blog post explores the exciting potential of AI in healthcare and its impact on the future of medicine.\n",
      "\n",
      "**H2: Transforming Patient Care**\n",
      "\n",
      "AI's ability to analyze vast amounts of data and identify patterns makes it a powerful tool for improving patient care. \n",
      "\n",
      "* **Personalized Medicine and Treatment:** AI algorithms can analyze individual patient data, including genetics, lifestyle, and medical history, to create personalized treatment plans. This approach allows for more effective and targeted therapies.\n",
      "* **Early Disease Detection and Prevention:** AI can analyze medical images, such as X-rays and MRIs, to detect early signs of diseases like\n",
      "\n",
      "=== SCORE ===\n",
      " 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.3` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "import re, torch, gc\n",
    "\n",
    "# ---------------- GPU cleanup (optional) ----------------\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "# ---------------- Load env ----------------\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------- Model ----------------\n",
    "gemma_model = \"google/gemma-2-2b-it\"\n",
    "\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=gemma_model,\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.3,       # a bit more deterministic for scoring\n",
    "        max_new_tokens=180,\n",
    "        return_full_text=False\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ---------------- State schema ----------------\n",
    "class BlogState(TypedDict):\n",
    "    title: str\n",
    "    outline: str\n",
    "    content: str\n",
    "    content_score: int  # 1..10\n",
    "\n",
    "# ---------------- Nodes ----------------\n",
    "def create_outline(state: BlogState) -> BlogState:\n",
    "    title = state[\"title\"]\n",
    "    prompt = (\n",
    "        \"Create a concise, hierarchical outline (H1/H2/H3) for a blog titled:\\n\"\n",
    "        f\"'{title}'. Keep it structured.\"\n",
    "    )\n",
    "    outline = model.invoke(prompt)\n",
    "    state[\"outline\"] = outline if isinstance(outline, str) else str(outline)\n",
    "    return state\n",
    "\n",
    "def create_blog(state: BlogState) -> BlogState:\n",
    "    title = state[\"title\"]\n",
    "    outline = state[\"outline\"]\n",
    "    prompt = (\n",
    "        \"Write a clear, well-structured blog post using the outline.\\n\"\n",
    "        f\"Title: {title}\\n\\nOutline:\\n{outline}\\n\\n\"\n",
    "        \"Use headings, short paragraphs, and end with 3 actionable takeaways.\"\n",
    "    )\n",
    "    content = model.invoke(prompt)\n",
    "    state[\"content\"] = content if isinstance(content, str) else str(content)\n",
    "    return state\n",
    "\n",
    "def score_content(state: BlogState) -> BlogState:\n",
    "    \"\"\"Ask the model for a 1..10 score and parse it robustly.\"\"\"\n",
    "    content = state[\"content\"]\n",
    "    prompt = (\n",
    "        \"Rate the quality of the following blog content on a scale of 1 to 10.\\n\"\n",
    "        \"Respond with ONLY the integer (no words, no punctuation).\\n\\n\"\n",
    "        f\"{content}\\n\\n\"\n",
    "        \"Score:\"\n",
    "    )\n",
    "    score_text = model.invoke(prompt)\n",
    "    score_text = score_text if isinstance(score_text, str) else str(score_text)\n",
    "\n",
    "    # Extract first integer 1..10\n",
    "    m = re.search(r\"\\b(10|[1-9])\\b\", score_text)\n",
    "    score = int(m.group(1)) if m else 0   # default to 0 if parsing fails\n",
    "    # Clamp just in case\n",
    "    score = max(0, min(10, score))\n",
    "    state[\"content_score\"] = score\n",
    "    return state\n",
    "\n",
    "# ---------------- Graph ----------------\n",
    "graph = StateGraph(BlogState)\n",
    "graph.add_node(\"create_outline\", create_outline)\n",
    "graph.add_node(\"create_blog\", create_blog)\n",
    "graph.add_node(\"score_content\", score_content)\n",
    "\n",
    "# Correct flow: END after scoring (remove the early END edge)\n",
    "graph.add_edge(START, \"create_outline\")\n",
    "graph.add_edge(\"create_outline\", \"create_blog\")\n",
    "graph.add_edge(\"create_blog\", \"score_content\")\n",
    "graph.add_edge(\"score_content\", END)\n",
    "\n",
    "workflow = graph.compile()\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "initial_state: BlogState = {\"title\": \"The Future of AI in Healthcare\", \"outline\": \"\", \"content\": \"\", \"content_score\": 0}\n",
    "final_state = workflow.invoke(initial_state)\n",
    "\n",
    "print(\"\\n=== OUTLINE ===\\n\", final_state[\"outline\"])\n",
    "print(\"\\n=== CONTENT ===\\n\", final_state[\"content\"])\n",
    "print(\"\\n=== SCORE ===\\n\", final_state[\"content_score\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
