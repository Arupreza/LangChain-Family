{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "499e43b3",
   "metadata": {},
   "source": [
    "Simmple LLM Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06a80973",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.67it/s]\n",
      "Device set to use cuda:0\n",
      "/home/lisa/anaconda3/envs/llm_env/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bangladesh is a country in South Asia. It is bordered by India to the west and the Bay of Bengal to the east. \n",
      "\n",
      "Please provide more context or information if you need it.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Imports\n",
    "from langgraph.graph import StateGraph, START, END   # StateGraph = workflow builder\n",
    "from langchain_huggingface import HuggingFacePipeline  # wrapper to use HF models in LangChain\n",
    "from typing import TypedDict   # define workflow state schema\n",
    "from dotenv import load_dotenv # load env variables (like Hugging Face token)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 1. Load environment variables\n",
    "# ---------------------------------------------\n",
    "# If your HuggingFace model requires an API token (gated/private models),\n",
    "# store it in a `.env` file as HUGGINGFACEHUB_API_TOKEN\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 2. Choose a Hugging Face model\n",
    "# ---------------------------------------------\n",
    "gemma_model = \"google/gemma-2-2b-it\"  # small, instruction-tuned model\n",
    "# llama_model = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # alternative demo model\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 3. Wrap the HF model as a LangChain LLM\n",
    "# ---------------------------------------------\n",
    "# HuggingFacePipeline.from_model_id loads the model + tokenizer\n",
    "# and returns a LangChain-compatible LLM object.\n",
    "model = HuggingFacePipeline.from_model_id(\n",
    "    model_id=gemma_model,\n",
    "    task=\"text-generation\",   # type of pipeline we want\n",
    "    pipeline_kwargs=dict(\n",
    "        temperature=0.5,      # controls randomness (lower = deterministic)\n",
    "        max_new_tokens=100,   # maximum tokens to generate per response\n",
    "        return_full_text=False # return only the answer (not the prompt+answer)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4. Define the workflow state schema\n",
    "# ---------------------------------------------\n",
    "# The workflow passes around a dictionary that must follow this schema.\n",
    "class LLMState(TypedDict):\n",
    "    user_input: str      # input question\n",
    "    model_response: str  # output answer (LLM result)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 5. Define a node function\n",
    "# ---------------------------------------------\n",
    "# Each node takes a state dict -> updates it -> returns it.\n",
    "def llm_qa(state: LLMState) -> LLMState:\n",
    "    # ✅ Extract the user question from the state\n",
    "    question = state[\"user_input\"]\n",
    "\n",
    "    # ✅ Create a simple prompt\n",
    "    prompt = (\n",
    "        \"Answer the question based on the context below.\\n\\n\"\n",
    "        f\"Context: {question}\\n\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "\n",
    "    # ✅ Call the Hugging Face model (works like a LangChain LLM)\n",
    "    result = model.invoke(prompt)\n",
    "\n",
    "    # ✅ Normalize output: usually it's already a string\n",
    "    state[\"model_response\"] = result.strip() if isinstance(result, str) else str(result).strip()\n",
    "\n",
    "    # ✅ Return updated state\n",
    "    return state\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 6. Build the LangGraph workflow\n",
    "# ---------------------------------------------\n",
    "graph = StateGraph(LLMState)          # create graph with our state schema\n",
    "graph.add_node(\"llm_qa\", llm_qa)      # add one node (our QA function)\n",
    "graph.add_edge(START, \"llm_qa\")       # connect START → llm_qa\n",
    "graph.add_edge(\"llm_qa\", END)         # connect llm_qa → END\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 7. Compile the graph into an executable workflow\n",
    "# ---------------------------------------------\n",
    "workflow = graph.compile()\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 8. Run the workflow with some input\n",
    "# ---------------------------------------------\n",
    "# Keys must match our LLMState schema: use 'user_input', not 'question'\n",
    "initial_state = {\"user_input\": \"Tell me about Bangladesh.\"}\n",
    "\n",
    "# Run the compiled workflow\n",
    "final_state = workflow.invoke(initial_state)\n",
    "\n",
    "# Print the result\n",
    "print(final_state['model_response'])\n",
    "# Example output:\n",
    "# {\n",
    "#   'user_input': 'Tell me about Bangladesh.',\n",
    "#   'model_response': 'Bangladesh is a country in South Asia...'\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a231a29",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
